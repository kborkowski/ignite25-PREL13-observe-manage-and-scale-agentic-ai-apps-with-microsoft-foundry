{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00a4ef, #7fba00, #ffb900, #f25022); padding: 20px; border-radius: 10px; text-align: left; color: black;\">\n",
    "    <h1> üîç | Step 0: Simulate Datasets For Evaluation </h1>\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "You want to evaluate the quality, safety, and agentic efficiency, of your application. To do this you need three things:\n",
    "1. A dataset of prompts - to serve as inputs\n",
    "2. A related set of context items - to serve as ground truth\n",
    "3. The actual response from the model/agent being evaluated \n",
    "\n",
    "In this section, we look at how the Simulator capability of the evaluation SDK can be used with a valid source (e.g., the Zava Products index) to identify a valid set of questions and grounding context. _This can then be fed to the model or agent being tested, to retrieve responses that can then be **judged** for various evaluation metrics.\n",
    "\n",
    "The simulator can _also_ collect responses from a targeted model - generating a dataset that has {question, response, context} triples that can be used for other purposes like fine-tuning.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to generate a synthetic dataset of queries and responses using your Azure Search index with the Simulator tool. The generated dataset can be useful for:\n",
    "\n",
    "- Testing and evaluating RAG workflows\n",
    "- Fine-tuning prompts\n",
    "- Benchmarking search capabilities\n",
    "- Creating synthetic training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Requisites\n",
    "\n",
    "1. An Azure OpenAI model deployment (chat completion)\n",
    "1. An Azure AI Search index (\"contoso-products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup Environment\n",
    "\n",
    "This section loads and validates all required Azure service credentials from environment variables. The code will:\n",
    "- Load environment variables from the `.env` file using `dotenv`\n",
    "- Check that all required Azure credentials are available\n",
    "- Initialize connection parameters for Azure AI Search\n",
    "- Configure the Azure OpenAI model for the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load environment variables and verify Azure services configuration\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify all required Azure service credentials are available\n",
    "required_vars = {\n",
    "    \"AZURE_OPENAI_API_KEY\": \"Azure OpenAI API key for chat completions\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\": \"Azure OpenAI service endpoint URL\", \n",
    "    \"AZURE_OPENAI_API_VERSION\": \"Azure OpenAI API version\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\": \"Azure OpenAI chat model deployment name\",\n",
    "    \"AZURE_SEARCH_ENDPOINT\": \"Azure AI Search service endpoint URL\",\n",
    "    \"AZURE_SEARCH_API_KEY\": \"Azure AI Search service API key\",\n",
    "    \"AZURE_SEARCH_INDEX_NAME\": \"Azure AI Search index name containing product data\"\n",
    "}\n",
    "\n",
    "print(\"üîç Checking environment configuration...\")\n",
    "missing_vars = []\n",
    "for var, description in required_vars.items():\n",
    "    if not os.environ.get(var):\n",
    "        missing_vars.append(f\"‚ùå {var}: {description}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {var}: Configured\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"\\n‚ö†Ô∏è Missing required environment variables:\")\n",
    "    for var in missing_vars:\n",
    "        print(var)\n",
    "    raise EnvironmentError(\"Please set all required environment variables in your .env file\")\n",
    "else:\n",
    "    print(\"\\nüéâ All environment variables are properly configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Initialize Azure AI Search connection parameters\n",
    "search_endpoint = os.environ.get(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_api_key = os.environ.get(\"AZURE_SEARCH_API_KEY\") \n",
    "search_index_name = os.environ.get(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "\n",
    "print(f\"üîé Azure AI Search Configuration:\")\n",
    "print(f\"   Endpoint: Configured ({search_endpoint.split('//')[1].split('.')[0] if search_endpoint else 'Not found'})\")\n",
    "print(f\"   Index: {search_index_name if search_index_name else 'Not configured'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Initialize the Simulator\n",
    "\n",
    "This section creates the Azure AI Evaluation Simulator that will generate synthetic datasets.\n",
    "\n",
    "### 2.1 Create a Model Configuration\n",
    "\n",
    "The code below creates an Azure OpenAI model configuration using environment variables. This configuration will be used by the simulator to generate synthetic queries and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configure Azure OpenAI model for the simulator\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Create model configuration using environment variables\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"), \n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Azure OpenAI Model Configuration:\")\n",
    "endpoint = model_config['azure_endpoint']\n",
    "print(f\"   Endpoint: Configured ({endpoint.split('//')[1].split('.')[0] if endpoint else 'Not found'})\")\n",
    "print(f\"   Deployment: {model_config['azure_deployment'] if model_config['azure_deployment'] else 'Not configured'}\")\n",
    "print(f\"   API Version: {model_config['api_version'] if model_config['api_version'] else 'Not configured'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Instantiate Simulator with the model\n",
    "\n",
    "This code creates the Azure AI Evaluation Simulator instance using the model configuration from above. The simulator will use this configuration to generate synthetic queries and responses for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize the Azure AI Evaluation Simulator\n",
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "# Create simulator instance with the configured model\n",
    "simulator = Simulator(model_config=model_config)\n",
    "print(\"üìä Simulator initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Connect to the Search Index\n",
    "\n",
    "This section establishes connection to Azure AI Search and defines functions to retrieve relevant content.\n",
    "\n",
    "### 3.1 Define function to retrieve search results for query\n",
    "\n",
    "The function below performs the following operations:\n",
    "- Constructs Azure AI Search API requests with proper authentication\n",
    "- Searches the index for relevant content based on user queries\n",
    "- Handles API responses and error conditions\n",
    "- Returns combined content from search results for use in RAG workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define function to search Azure AI Search index and retrieve content\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def search_index_for_content(query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search the Azure AI Search index for relevant content based on a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query to find relevant content\n",
    "        top_k (int): Number of top results to retrieve (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        str: Combined content from search results, truncated to 1000 characters\n",
    "    \"\"\"\n",
    "    # Construct the search API endpoint\n",
    "    search_url = f\"{search_endpoint}/indexes/{search_index_name}/docs/search?api-version=2023-11-01\"\n",
    "    \n",
    "    # Set up request headers with API key authentication\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": search_api_key\n",
    "    }\n",
    "    \n",
    "    # Define the search query payload\n",
    "    search_payload = {\n",
    "        \"search\": query,\n",
    "        \"top\": top_k,\n",
    "        \"select\": \"content,title\"  # Updated to match available fields\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Execute the search request\n",
    "        response = requests.post(url=search_url, headers=headers, json=search_payload)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP error codes\n",
    "        \n",
    "        # Parse the search results\n",
    "        search_results = response.json()\n",
    "        combined_content = \"\"\n",
    "        \n",
    "        # Extract and combine content from search results\n",
    "        for result in search_results.get(\"value\", []):\n",
    "            # Prioritize 'content' field, fall back to 'title'\n",
    "            content = result.get(\"content\") or result.get(\"title\", \"\")\n",
    "            if content:\n",
    "                combined_content += content + \" \"\n",
    "        \n",
    "        # Limit content length to prevent token limits and improve performance\n",
    "        return combined_content[:1000].strip()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error searching index: {e}\")\n",
    "        return f\"Error retrieving content for query: {query}\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Error parsing search response: {e}\")\n",
    "        return f\"Error processing search results for query: {query}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test the function works with a query\n",
    "\n",
    "This cell tests the search function with a sample query to verify that:\n",
    "- The Azure AI Search connection is working properly\n",
    "- The search index contains retrievable content\n",
    "- The function returns relevant results for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Test the search function with a sample query\n",
    "test_query = \"spray paint\"  # Get all documents to see what's available\n",
    "retrieved_content = search_index_for_content(test_query)\n",
    "\n",
    "print(f\"üîç Test search for: '{test_query}'\")\n",
    "print(f\"üìÑ Retrieved content length: {len(retrieved_content)} characters\")\n",
    "print(f\"üìù Sample content preview:\")\n",
    "print(\"-\" * 50)\n",
    "print(retrieved_content[:300] + \"...\" if len(retrieved_content) > 300 else retrieved_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Create Application Callback\n",
    "\n",
    "This section defines the RAG (Retrieval-Augmented Generation) application callback that the simulator will use to generate responses.\n",
    "\n",
    "The callback function:\n",
    "- Extracts user queries from conversation messages\n",
    "- Searches the Azure AI Search index for relevant context\n",
    "- Uses Azure OpenAI to generate responses based on retrieved content\n",
    "- Returns properly formatted responses for the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define the application callback function for the simulator\n",
    "from typing import Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "async def rag_application_callback(\n",
    "    messages: Dict,\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Callback function that simulates a RAG (Retrieval-Augmented Generation) application.\n",
    "    \n",
    "    This function:\n",
    "    1. Extracts the user query from the message\n",
    "    2. Searches the Azure AI Search index for relevant content\n",
    "    3. Uses Azure OpenAI to generate a response based on the retrieved content\n",
    "    4. Returns the response in the expected format\n",
    "    \n",
    "    Args:\n",
    "        messages (Dict): Message history containing user queries\n",
    "        stream (bool): Whether to stream the response (not used in this implementation)\n",
    "        session_state (Any): Session state information\n",
    "        context (Optional[Dict[str, Any]]): Additional context information\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Response containing the generated message and metadata\n",
    "    \"\"\"\n",
    "    # Extract the user's query from the latest message\n",
    "    messages_list = messages[\"messages\"]\n",
    "    user_query = messages_list[-1][\"content\"]\n",
    "    \n",
    "    # Initialize Azure OpenAI client\n",
    "    openai_client = AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n",
    "    \n",
    "    # Retrieve relevant content from the search index\n",
    "    retrieved_context = search_index_for_content(user_query)\n",
    "    \n",
    "    # Create a system prompt that instructs the model to use the retrieved context\n",
    "    system_prompt = \"\"\"You are a polite and helpful assistant that answers questions based on the provided context. \n",
    "Use the context information to provide accurate and relevant responses. If the context doesn't contain \n",
    "enough information to answer the question, say so politely. If the context mentions a product by name, reference it in the response.\"\"\"\n",
    "    \n",
    "    # Generate response using Azure OpenAI\n",
    "    try:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "            model=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {retrieved_context}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {user_query}\"}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        # Extract the generated response\n",
    "        ai_response = completion.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        ai_response = f\"Sorry, I encountered an error while generating a response: {str(e)}\"\n",
    "    \n",
    "    # Format the response according to the expected structure\n",
    "    response_message = {\n",
    "        \"content\": ai_response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": retrieved_context,\n",
    "    }\n",
    "    \n",
    "    # Add the response to the message history\n",
    "    messages[\"messages\"].append(response_message)\n",
    "    \n",
    "    # Return the complete response structure\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"], \n",
    "        \"stream\": stream, \n",
    "        \"session_state\": session_state, \n",
    "        \"context\": retrieved_context\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Generate & Save Dataset\n",
    "\n",
    "This section runs the simulator to create synthetic query-response pairs and saves them for evaluation.\n",
    "\n",
    "### 5.1 Define tasks and run the simulator\n",
    "\n",
    "The code below:\n",
    "- Uses content retrieved from the search index as seed material\n",
    "- Generates realistic queries based on the content\n",
    "- Creates responses using the RAG application callback\n",
    "- Produces a dataset of query-response pairs with context for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Generate synthetic dataset using the simulator\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üéØ Starting dataset generation...\")\n",
    "print(\"This process will:\")\n",
    "print(\"1. Use content from your search index to generate realistic queries\")\n",
    "print(\"2. Generate responses using your RAG application callback\")\n",
    "print(\"3. Create query-response pairs for evaluation purposes\")\n",
    "print()\n",
    "\n",
    "# Run the simulator to generate synthetic data\n",
    "# Note: Using the retrieved_content from our previous test as seed content\n",
    "synthetic_outputs = await simulator(\n",
    "    target=rag_application_callback,  # Our RAG application function\n",
    "    text=retrieved_content,           # Seed content from the search index\n",
    "    num_queries=5,                    # Number of query-response pairs to generate\n",
    "    max_conversation_turns=1,         # Keep conversations simple (single turn)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(synthetic_outputs)} synthetic query-response pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Save the generated dataset\n",
    "\n",
    "This code saves the generated synthetic dataset to a JSONL file for use in evaluation workflows. Each line contains a query-response pair with context information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save the generated dataset to file\n",
    "output_file = Path(\"lab-simulate-datasets.jsonl\")\n",
    "\n",
    "print(f\"üíæ Saving dataset to: {output_file.absolute()}\")\n",
    "\n",
    "# Write each output as a JSON line to the file\n",
    "with output_file.open(\"w\") as f:\n",
    "    for output in synthetic_outputs:\n",
    "        f.write(output.to_eval_qr_json_lines())\n",
    "\n",
    "print(f\"‚úÖ Dataset successfully saved!\")\n",
    "print(f\"üìÅ File location: {output_file.absolute()}\")\n",
    "print(f\"üìä Total records: {len(synthetic_outputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Review the generated dataset\n",
    "\n",
    "This code loads and displays a preview of the generated dataset using pandas to help verify:\n",
    "- The dataset structure and format\n",
    "- Sample query-response pairs\n",
    "- Data quality and relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Preview the generated dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üìã Dataset Preview:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load and display the first few records\n",
    "try:\n",
    "    dataset_df = pd.read_json(output_file, lines=True)\n",
    "    \n",
    "    # Display basic dataset information\n",
    "    print(f\"Dataset shape: {dataset_df.shape}\")\n",
    "    print(f\"Columns: {list(dataset_df.columns)}\")\n",
    "    print()\n",
    "    \n",
    "    # Show first few records with limited content for readability\n",
    "    display_df = dataset_df.head(3).copy()\n",
    "    \n",
    "    # Truncate long content for better display\n",
    "    for col in display_df.columns:\n",
    "        if display_df[col].dtype == 'object':\n",
    "            display_df[col] = display_df[col].apply(\n",
    "                lambda x: str(x)[:100] + \"...\" if len(str(x)) > 100 else str(x)\n",
    "            )\n",
    "    \n",
    "    display_df\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading dataset: {e}\")\n",
    "    print(\"The dataset file may not have been created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Review the saved dataset file\n",
    "\n",
    "Manual inspection step:\n",
    "- Open the `lab-simulated-datasets.jsonl` file in your Visual Studio Code editor\n",
    "- Examine the structure of generated {query-response-context} lines \n",
    "- Verify that the synthetic data is relevant and useful for evaluation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Next Steps\n",
    "\n",
    "Now that you have generated a synthetic evaluation dataset, you can:\n",
    "\n",
    "1. **Evaluate retrieval quality** - Use the dataset to test how well your search index retrieves relevant information\n",
    "2. **Fine-tune prompts** - Analyze common query patterns to improve your system prompts\n",
    "3. **Create test cases** - Use generated queries as test cases for your RAG application\n",
    "4. **Identify improvements** - Analyze the dataset to find areas where your application could be enhanced\n",
    "5. **Benchmark performance** - Establish baseline metrics for your RAG system using this synthetic data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
