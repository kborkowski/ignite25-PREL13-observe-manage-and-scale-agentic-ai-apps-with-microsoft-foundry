{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Selection & Evaluation\n",
    "\n",
    "The first stage of the AI lifecycle involves selecting an appropriate base model. Generative AI models vary widely in terms of capabilities, strengths, and limitations, so it's essential to identify which model best suits your specific use case. \n",
    "\n",
    "In this notebook, we teach you how to use the Evaluation SDK to assess multiple model candidates using the same _set of evaluators_ - then reviewing the results to see which model fits our needs best. This lets you tailor your evaluation to specific metrics.\n",
    "\n",
    "Alternatively, you can use the [Leaderboards](https://ai.azure.com/leaderboards) to compare models using standard industry benchmarks, for quality, safety and costs.\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Required Python Packages\n",
    "\n",
    "The dev container has already installed all the necessary Python packages for you:\n",
    "- `azure-ai-evaluation`: The main SDK for running evaluations\n",
    "- `azure-identity`: For authentication with Azure\n",
    "- `pandas`: For data manipulation and analysis\n",
    "\n",
    "Let's verify these packages are available and check their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required packages are installed\n",
    "import importlib.metadata\n",
    "\n",
    "try:\n",
    "    azure_eval_version = importlib.metadata.version('azure-ai-evaluation')\n",
    "    azure_identity_version = importlib.metadata.version('azure-identity')\n",
    "    pandas_version = importlib.metadata.version('pandas')\n",
    "    \n",
    "    print(\"‚úÖ All required packages are installed!\")\n",
    "    print(f\"üì¶ azure-ai-evaluation: {azure_eval_version}\")\n",
    "    print(f\"üì¶ azure-identity: {azure_identity_version}\")\n",
    "    print(f\"üì¶ pandas: {pandas_version}\")\n",
    "except importlib.metadata.PackageNotFoundError as e:\n",
    "    print(f\"‚ùå Missing package: {e}\")\n",
    "    print(\"Please ensure the dev container has been properly set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Environment Variables\n",
    "\n",
    "Let's load the environment variables from the `.env` file. These variables should already be configured from the initial lab setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded from .env file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Required Environment Variables\n",
    "\n",
    "Before we proceed, let's verify that all the required environment variables are set:\n",
    "\n",
    "1. **AZURE_OPENAI_ENDPOINT**: Your Azure OpenAI endpoint\n",
    "2. **AZURE_OPENAI_API_VERSION**: The API version for Azure OpenAI\n",
    "3. **AZURE_OPENAI_API_KEY**: Your Azure OpenAI API key\n",
    "4. **AZURE_OPENAI_DEPLOYMENT**: The deployment name for the judge model\n",
    "5. **AZURE_SUBSCRIPTION_ID**: Your Azure subscription ID\n",
    "6. **AZURE_RESOURCE_GROUP**: Your Azure resource group name\n",
    "7. **AZURE_AI_PROJECT_NAME**: Your Azure AI project name\n",
    "\n",
    "> **Note**: These environment variables should already be set from the initial lab setup in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_env_variables(env_vars):\n",
    "    undefined_vars = [var for var in env_vars if os.getenv(var) is None]\n",
    "    if undefined_vars:\n",
    "        print(f\"‚ùå The following environment variables are not defined: {', '.join(undefined_vars)}\")\n",
    "        raise ValueError(f\"Missing required environment variables: {', '.join(undefined_vars)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required environment variables are defined.\")\n",
    "\n",
    "# Let's check required env variables for this exercise\n",
    "env_vars_to_check = [\n",
    "    'AZURE_OPENAI_API_KEY', \n",
    "    'AZURE_OPENAI_ENDPOINT', \n",
    "    'AZURE_OPENAI_DEPLOYMENT', \n",
    "    'AZURE_SUBSCRIPTION_ID', \n",
    "    'AZURE_RESOURCE_GROUP', \n",
    "    'AZURE_AI_PROJECT_NAME'\n",
    "]\n",
    "check_env_variables(env_vars_to_check)\n",
    "\n",
    "# Print configuration for verification\n",
    "print(f\"\\nüìç Azure OpenAI Endpoint: {os.environ.get('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"üìã API Version: {os.environ.get('AZURE_OPENAI_API_VERSION', 'Not set - will use default')}\")\n",
    "print(f\"‚öñÔ∏è  Judge Model Deployment: {os.environ.get('AZURE_OPENAI_DEPLOYMENT')}\")\n",
    "print(f\"üìÇ Project Name: {os.environ.get('AZURE_AI_PROJECT_NAME')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Azure AI Project Configuration\n",
    "\n",
    "Let's create the Azure AI Project configuration object that will be used to upload evaluation results to the Azure AI Foundry portal for viewing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Get Azure AI project configuration from environment variables\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "\n",
    "# Create the azure_ai_project configuration\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Azure AI Foundry project configuration created!\")\n",
    "print(\"=\"*80)\n",
    "pprint(azure_ai_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Test Data\n",
    "\n",
    "Let's load the test data from `data.jsonl`. This file contains:\n",
    "- **query**: Questions to ask the model\n",
    "- **context**: Background information for answering the question\n",
    "- **ground_truth**: Expected correct answers for comparison\n",
    "\n",
    "This data will be used to evaluate how well each model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# Load the test data\n",
    "data_file = \"data.jsonl\"\n",
    "data_path = str(pathlib.Path(pathlib.Path.cwd())) + f\"/{data_file}\"\n",
    "\n",
    "# Read and display the data\n",
    "df = pd.read_json(data_file, lines=True)\n",
    "\n",
    "print(f\"‚úÖ Test data loaded successfully!\")\n",
    "print(f\"üìÑ Data file: {data_path}\")\n",
    "print(f\"üìä Number of test cases: {len(df)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã SAMPLE DATA:\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure the Judge Model\n",
    "\n",
    "To evaluate model responses, we need an \"LLM Judge\" - another AI model that will assess the quality of the responses. We'll configure the judge model using a simple dictionary configuration.\n",
    "\n",
    "The judge model will be used by evaluators like:\n",
    "- **RelevanceEvaluator**: Checks if the response is relevant to the question\n",
    "- **CoherenceEvaluator**: Checks if the response is logically structured\n",
    "\n",
    "This is also known as \"LLM as a Judge\" evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the judge model using the dictionary approach (as shown in sample-eval.ipynb)\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Judge model configured successfully!\")\n",
    "print(\"=\"*80)\n",
    "pprint(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Initialize the Evaluators\n",
    "\n",
    "We'll use multiple types of evaluators to comprehensively assess model performance:\n",
    "\n",
    "**LLM as Judge Evaluators** (require judge model):\n",
    "- **RelevanceEvaluator**: Measures how relevant the response is to the query\n",
    "- **CoherenceEvaluator**: Measures logical flow and readability\n",
    "\n",
    "**NLP Evaluators** (no judge model needed):\n",
    "- **BleuScoreEvaluator**: Compares response to ground truth using n-gram overlap\n",
    "- **RougeScoreEvaluator**: Measures overlap of words/phrases with ground truth\n",
    "\n",
    "**Content Safety Evaluators** (Azure AI service):\n",
    "- **ViolenceEvaluator**: Detects violent or harmful content\n",
    "\n",
    "This combination gives us a comprehensive view of quality, accuracy, and safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator, \n",
    "    CoherenceEvaluator, \n",
    "    BleuScoreEvaluator, \n",
    "    RougeScoreEvaluator, \n",
    "    RougeType, \n",
    "    ViolenceEvaluator,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# LLM as Judge evaluators (require judge model)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "# NLP evaluators (no model required)\n",
    "bleu_score_evaluator = BleuScoreEvaluator()\n",
    "rouge_score_evaluator = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)\n",
    "\n",
    "# Content Safety evaluator (requires Azure AI project)\n",
    "violence_evaluator = ViolenceEvaluator(\n",
    "    azure_ai_project=azure_ai_project, \n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All evaluators initialized successfully!\")\n",
    "print(\"üìä Evaluators configured:\")\n",
    "print(\"   - Relevance (LLM Judge)\")\n",
    "print(\"   - Coherence (LLM Judge)\")\n",
    "print(\"   - BLEU Score (NLP)\")\n",
    "print(\"   - ROUGE Score (NLP)\")\n",
    "print(\"   - Violence (Content Safety)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Target Function for Model Evaluation\n",
    "\n",
    "The target function is what the evaluation SDK calls to get responses from your model. It receives a query from the test data and returns the model's response.\n",
    "\n",
    "We'll create a callable class that:\n",
    "1. Accepts a model deployment name in the constructor\n",
    "2. Implements a `__call__` method that takes a query and returns a response\n",
    "3. Uses Azure OpenAI to generate responses\n",
    "\n",
    "This allows us to easily test different models by creating instances with different deployment names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "class ModelTarget:\n",
    "    \"\"\"\n",
    "    A callable target class for model evaluation.\n",
    "    This class wraps an Azure OpenAI model and provides a simple interface\n",
    "    for the evaluation SDK to call and get responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_deployment_name: str):\n",
    "        \"\"\"\n",
    "        Initialize the model target with a specific deployment.\n",
    "        \n",
    "        Args:\n",
    "            model_deployment_name: The name of the Azure OpenAI model deployment\n",
    "        \"\"\"\n",
    "        self.model_deployment_name = model_deployment_name\n",
    "        \n",
    "        # Create Azure OpenAI client\n",
    "        self.client = AzureOpenAI(\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "        \n",
    "    def __call__(self, query: str) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a response for the given query.\n",
    "        \n",
    "        Args:\n",
    "            query: The question or prompt from the test data\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary with the 'response' key containing the model's answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Call Azure OpenAI with the query\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model_deployment_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Provide clear, accurate, and concise responses.\"},\n",
    "                    {\"role\": \"user\", \"content\": query}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=800\n",
    "            )\n",
    "            \n",
    "            # Extract the response text\n",
    "            response_text = completion.choices[0].message.content\n",
    "            \n",
    "            return {\"response\": response_text}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calling model {self.model_deployment_name}: {e}\")\n",
    "            return {\"response\": f\"Error: {str(e)}\"}\n",
    "\n",
    "print(\"‚úÖ ModelTarget class defined successfully!\")\n",
    "print(\"üí° This class will be used to generate responses from different models for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Evaluation Function\n",
    "\n",
    "Now let's create a reusable function that:\n",
    "1. Takes a model deployment name as input\n",
    "2. Creates a `ModelTarget` instance for that model\n",
    "3. Runs the evaluation with all configured evaluators\n",
    "4. Returns the results\n",
    "\n",
    "This function will make it easy to evaluate multiple models with the same test data and evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_deployment_name: str):\n",
    "    \"\"\"\n",
    "    Evaluate a specific model deployment using the configured evaluators.\n",
    "    \n",
    "    Args:\n",
    "        model_deployment_name: The name of the Azure OpenAI deployment to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation results including metrics and a link to Azure AI Foundry\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîÑ Starting evaluation for: {model_deployment_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create the target for this model\n",
    "    model_target = ModelTarget(model_deployment_name)\n",
    "    \n",
    "    # Run the evaluation\n",
    "    results = evaluate(\n",
    "        evaluation_name=f\"Base Model Evaluation - {model_deployment_name}\",\n",
    "        data=data_path,\n",
    "        target=model_target,\n",
    "        evaluators={\n",
    "            \"relevance\": relevance_evaluator,\n",
    "            \"coherence\": coherence_evaluator,\n",
    "            \"bleu_score\": bleu_score_evaluator,\n",
    "            \"rouge_score\": rouge_score_evaluator,\n",
    "            \"violence_score\": violence_evaluator,\n",
    "        },\n",
    "        azure_ai_project=azure_ai_project,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation completed for {model_deployment_name}!\")\n",
    "    print(f\"üìä Results uploaded to Azure AI Foundry\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined successfully!\")\n",
    "print(\"üöÄ Ready to evaluate models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate All Deployed Models\n",
    "\n",
    "Now let's evaluate all your deployed models systematically. We'll run the same evaluation on each model to compare their performance side-by-side.\n",
    "\n",
    "**Models to evaluate:**\n",
    "- **gpt-4.1**: Latest GPT-4 model with enhanced capabilities\n",
    "- **gpt-4.1-mini**: Efficient version of GPT-4.1\n",
    "- **gpt-4.1-nano**: Ultra-efficient version for cost-sensitive workloads\n",
    "- **gpt-4o**: Optimized GPT-4 variant (2024-11-20)\n",
    "- **gpt-4o-mini**: Smaller, faster version of GPT-4o\n",
    "\n",
    "The evaluation will measure:\n",
    "- How relevant the responses are\n",
    "- How coherent and well-structured they are\n",
    "- How closely they match the ground truth\n",
    "- Whether they contain any harmful content\n",
    "\n",
    "> **Note**: This may take several minutes as we evaluate multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all deployed models to evaluate\n",
    "models_to_evaluate = [\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\"\n",
    "]\n",
    "\n",
    "# Dictionary to store all evaluation results\n",
    "all_results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name in models_to_evaluate:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ Evaluating model: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        results = evaluate_model(model_name)\n",
    "        all_results[model_name] = results\n",
    "        \n",
    "        # Display summary\n",
    "        results_df = pd.DataFrame(results[\"rows\"])\n",
    "        print(f\"\\nüìä Results for {model_name}:\")\n",
    "        print(results_df.head())\n",
    "        print(f\"\\nüåê View in portal: {results['studio_url']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluating {model_name}: {e}\")\n",
    "        all_results[model_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL MODEL EVALUATIONS COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Successfully evaluated {len([r for r in all_results.values() if r is not None])} out of {len(models_to_evaluate)} models\")\n",
    "print(\"\\nüí° Results are stored both locally and in Azure AI Foundry portal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Analyze & Compare Results (Local + Cloud)\n",
    "\n",
    "Now let's analyze the evaluation results both **locally** (in this notebook) and in the **Azure AI Foundry portal** for comprehensive comparison.\n",
    "\n",
    "### üìä Local Analysis\n",
    "\n",
    "We'll aggregate metrics across all models to identify trends and patterns right here in the notebook.\n",
    "\n",
    "### üåê Cloud Portal Analysis\n",
    "\n",
    "We'll also provide links to view detailed dashboards in Azure AI Foundry where you can:\n",
    "- Compare side-by-side metrics\n",
    "- View interactive charts\n",
    "- Drill down into individual test cases\n",
    "- Export results for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# LOCAL ANALYSIS: Aggregate metrics across all models\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä LOCAL ANALYSIS: MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    if results is not None:\n",
    "        # Extract metrics from results\n",
    "        metrics = results.get(\"metrics\", {})\n",
    "        \n",
    "        comparison_data.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Relevance\": f\"{metrics.get('relevance.gpt_relevance', 0):.2f}\",\n",
    "            \"Coherence\": f\"{metrics.get('coherence.gpt_coherence', 0):.2f}\",\n",
    "            \"BLEU Score\": f\"{metrics.get('bleu_score.bleu_score', 0):.3f}\",\n",
    "            \"ROUGE Score\": f\"{metrics.get('rouge_score.rouge_score', 0):.3f}\",\n",
    "            \"Violence\": f\"{metrics.get('violence_score.violence_score', 0):.2f}\",\n",
    "        })\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best performing models\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"üèÜ BEST PERFORMERS BY METRIC:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if len(comparison_data) > 0:\n",
    "    # Convert string values back to float for comparison\n",
    "    for metric in [\"Relevance\", \"Coherence\", \"BLEU Score\", \"ROUGE Score\"]:\n",
    "        values = [(row[\"Model\"], float(row[metric])) for row in comparison_data]\n",
    "        best_model, best_score = max(values, key=lambda x: x[1])\n",
    "        print(f\"  {metric}: {best_model} ({best_score:.3f})\")\n",
    "    \n",
    "    # For violence, lower is better\n",
    "    violence_values = [(row[\"Model\"], float(row[\"Violence\"])) for row in comparison_data]\n",
    "    safest_model, lowest_violence = min(violence_values, key=lambda x: x[1])\n",
    "    print(f\"  Safety (Violence): {safest_model} ({lowest_violence:.3f})\")\n",
    "\n",
    "# Save results locally\n",
    "output_file = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    # Save all results with metrics\n",
    "    output_data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"models_evaluated\": models_to_evaluate,\n",
    "        \"comparison_summary\": comparison_data,\n",
    "        \"detailed_results\": {\n",
    "            model: {\n",
    "                \"metrics\": results.get(\"metrics\", {}) if results else None,\n",
    "                \"studio_url\": results.get(\"studio_url\", \"\") if results else None\n",
    "            }\n",
    "            for model, results in all_results.items()\n",
    "        }\n",
    "    }\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Local results saved to: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLOUD PORTAL ANALYSIS: Links to Azure AI Foundry\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üåê CLOUD PORTAL ANALYSIS: Azure AI Foundry Dashboard\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üìã View detailed results for each model in the portal:\\n\")\n",
    "for model_name, results in all_results.items():\n",
    "    if results is not None:\n",
    "        print(f\"  ‚Ä¢ {model_name}:\")\n",
    "        print(f\"    {results['studio_url']}\\n\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"üìä TO COMPARE ALL MODELS IN THE PORTAL:\")\n",
    "print(\"-\"*80)\n",
    "print(\"\"\"\n",
    "1. Navigate to the Evaluations tab in Azure AI Foundry portal\n",
    "2. You should see evaluation runs for all models:\n",
    "   - Base Model Evaluation - gpt-4.1\n",
    "   - Base Model Evaluation - gpt-4.1-mini\n",
    "   - Base Model Evaluation - gpt-4.1-nano\n",
    "   - Base Model Evaluation - gpt-4o\n",
    "   - Base Model Evaluation - gpt-4o-mini\n",
    "\n",
    "3. Select the runs you want to compare (check the boxes)\n",
    "\n",
    "4. Click \"Switch to Dashboard View\" for visual comparison\n",
    "\n",
    "5. Use the Charts tab to see:\n",
    "   - Color-coded performance metrics\n",
    "   - Side-by-side comparisons\n",
    "   - Trend analysis across evaluation criteria\n",
    "\n",
    "üí° TIP: Compare models by family (e.g., all gpt-4.1 variants) to see \n",
    "        how mini/nano versions trade performance for efficiency!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Local summary: {len(comparison_data)} models analyzed\")\n",
    "print(f\"üíæ Results saved: {output_file}\")\n",
    "print(f\"üåê Portal URLs: Ready for cloud comparison\")\n",
    "print(\"\\nüí° You now have both local metrics and cloud dashboards for comprehensive analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed the Base Model Selection & Evaluation tutorial! Here's what you learned:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. ‚úÖ **Package Verification**: Confirmed all required packages are installed\n",
    "2. ‚úÖ **Environment Configuration**: Loaded variables from .env file using environment variables\n",
    "3. ‚úÖ **Azure AI Project Setup**: Extracted project details from connection string\n",
    "4. ‚úÖ **Test Data Loading**: Loaded and reviewed evaluation test cases\n",
    "5. ‚úÖ **Judge Model Configuration**: Set up an LLM judge for evaluation (BASE_MODELS_JUDGE)\n",
    "6. ‚úÖ **Evaluator Initialization**: Configured multiple types of evaluators (quality, NLP, safety)\n",
    "7. ‚úÖ **Target Function Creation**: Built a reusable ModelTarget class for testing\n",
    "8. ‚úÖ **Model Evaluation**: Evaluated 5 different models with the same test data\n",
    "9. ‚úÖ **Local & Cloud Analysis**: Analyzed results both locally and in Azure AI Foundry portal\n",
    "10. ‚úÖ **Results Storage**: Saved evaluation data locally (JSON) and to the cloud\n",
    "\n",
    "### Environment Variables Used:\n",
    "- `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint\n",
    "- `AZURE_OPENAI_API_VERSION`: API version for Azure OpenAI\n",
    "- `AZURE_OPENAI_API_KEY`: Authentication for Azure OpenAI\n",
    "- `AZURE_AI_CONNECTION_STRING`: Azure AI Foundry project connection\n",
    "- `BASE_MODELS_JUDGE`: Judge model deployment for LLM-based evaluation\n",
    "\n",
    "### Evaluators Explored:\n",
    "- **Relevance**: Measures response relevance to query (LLM Judge)\n",
    "- **Coherence**: Measures logical flow and readability (LLM Judge)\n",
    "- **BLEU Score**: N-gram overlap with ground truth (NLP)\n",
    "- **ROUGE Score**: Word/phrase overlap with ground truth (NLP)\n",
    "- **Violence**: Content safety evaluation (Azure AI)\n",
    "\n",
    "### Models Evaluated:\n",
    "- ü§ñ **gpt-4.1**: Latest GPT-4 model with enhanced capabilities (capacity: 50)\n",
    "- ü§ñ **gpt-4.1-mini**: Efficient version of GPT-4.1 (capacity: 20)\n",
    "- ü§ñ **gpt-4.1-nano**: Ultra-efficient version for cost-sensitive workloads (capacity: 20)\n",
    "- ü§ñ **gpt-4o**: Optimized GPT-4 variant from Nov 2024 (capacity: 20)\n",
    "- ü§ñ **gpt-4o-mini**: Smaller, faster version of GPT-4o\n",
    "\n",
    "### What You Learned:\n",
    "- How to compare multiple models systematically\n",
    "- How to use different types of evaluators for comprehensive assessment\n",
    "- How to analyze results locally with aggregated metrics\n",
    "- How to leverage Azure AI Foundry portal for visual comparisons\n",
    "- How to save evaluation results in both local files and cloud storage\n",
    "- How to interpret evaluation metrics for informed model selection\n",
    "\n",
    "### Results Storage:\n",
    "- **Local**: JSON file with timestamp (evaluation_results_YYYYMMDD_HHMMSS.json)\n",
    "- **Cloud**: Azure AI Foundry portal with interactive dashboards\n",
    "- **Format**: Structured data with metrics, URLs, and comparison summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
