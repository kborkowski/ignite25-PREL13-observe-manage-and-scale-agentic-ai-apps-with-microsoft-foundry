{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b6e53",
   "metadata": {},
   "source": [
    "# Lab 02: Explore Built-in Quality Evaluators\n",
    "\n",
    "By the end of this lab, you will know:\n",
    "\n",
    "1. What AI-Assisted evaluation workflows are, and how to run them.\n",
    "1. The built-in quality evaluators available in Azure AI Foundry\n",
    "1. How to run a quality evaluator with a test prompt (to understand usage)\n",
    "1. How to run a composite quality evaluator (with multiple evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c2dc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Create Model Configuration Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for AI-assisted evaluators\n",
    "# in Foundry projects\n",
    "\n",
    "import os\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a518b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Quality Evaluators\n",
    "\n",
    "Scores are typically numerical, generated using a Likert scale (1 to 5) with higher scores indicating better quality. The _threshold_ sets the cutoff for a \"pass/fail\" rating on that evaluator, helping you get a quick sense of where the primary issues lie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9514f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Coherence\n",
    "# ............\n",
    "# CoherenceEvaluator measures the logical and orderly presentation of ideas in a response, \n",
    "# allowing the reader to easily follow and understand the writer's train of thought. \n",
    "# A coherent response directly addresses the question with clear connections between \n",
    "# sentences and paragraphs, using appropriate transitions and a logical sequence of ideas. \n",
    "# Higher scores mean better coherence.\n",
    "\n",
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "\n",
    "coherence = CoherenceEvaluator(model_config=model_config, threshold=3)\n",
    "coherence(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a139730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Fluency\n",
    "# ..........\n",
    "# FluencyEvaluator measures the effectiveness and clarity of written communication, \n",
    "# focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, \n",
    "# and overall readability. It assesses how smoothly ideas are conveyed and how easily \n",
    "# the reader can understand the text.\n",
    "\n",
    "from azure.ai.evaluation import FluencyEvaluator\n",
    "\n",
    "fluency = FluencyEvaluator(model_config=model_config, threshold=3)\n",
    "fluency(\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a17578",
   "metadata": {},
   "source": [
    "## 3. Composite QA Evaluator\n",
    "QAEvaluator measures comprehensively various aspects in a question-answering scenario - including Relevance, Groundedness, Fluency, Coherence, Similarity, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea81a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 QA Evaluation\n",
    "# .................\n",
    "# QAEvaluator measures comprehensively various aspects \n",
    "# in a question-answering scenario:\n",
    "# Relevance / Groundedness / Fluency / Coherence\n",
    "# Similarity / F1 score\n",
    "\n",
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "qa_eval = QAEvaluator(model_config=model_config, threshold=3)\n",
    "qa_eval(\n",
    "    query=\"Where was Marie Curie born?\", \n",
    "    context=\"Background: 1. Marie Curie was a chemist. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist.\",\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b234",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.  Retrieval Augmented Generation (RAG) Evaluators\n",
    "\n",
    "A retrieval-augmented generation (RAG) system tries to generate the most relevant answer consistent with grounding documents in response to a user's query.  This requires it to _retrieve_ documents that provide grounding context, and _generate_ responses that are relevance, consistent with grounding data, and complete.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2c47f",
   "metadata": {},
   "source": [
    "### 3.1 Retrieval Evaluator\n",
    "RetrievalEvaluator measures the textual quality of retrieval results with an LLM without requiring ground truth. This metric focuses on how relevant the context chunks (encoded as a string) are to address a query and how the most relevant context chunks are surfaced at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65955c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RetrievalEvaluator\n",
    "\n",
    "retrieval = RetrievalEvaluator(model_config=model_config, threshold=3)\n",
    "retrieval(\n",
    "    query=\"Where was Marie Curie born?\", \n",
    "    context=\"Background: 1. Marie Curie was born in Warsaw. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eff3",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Groundedness Evaluator \n",
    "GroundednessEvaluator measures how well the generated response aligns with the given context (grounding source) and doesn't fabricate content outside of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness = GroundednessEvaluator(model_config=model_config, threshold=3)\n",
    "groundedness(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    context=\"Background: 1. Marie Curie is born on November 7, 1867. 2. Marie Curie is born in Warsaw.\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433c969",
   "metadata": {},
   "source": [
    "### 3.3 Relevance Evaluator\n",
    "\n",
    "RelevanceEvaluator measures how effectively a response addresses a query. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given query. Higher scores mean better relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44116930",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"relevance\": 4.0,\n",
    "    \"gpt_relevance\": 4.0, \n",
    "    \"relevance_reason\": \"The RESPONSE accurately answers the QUERY by stating that Marie Curie was born in Warsaw, which is correct and directly relevant to the question asked.\",\n",
    "    \"relevance_result\": \"pass\", \n",
    "    \"relevance_threshold\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "result = coherence_evaluator(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179ed8f",
   "metadata": {},
   "source": [
    "### 3.4 Response Completeness Evaluator\n",
    "\n",
    "ResponseCompletenessEvaluator that captures the recall aspect of response alignment with the expected response. This is complementary to GroundednessEvaluator which captures the precision aspect of response alignment with the grounding source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "response_completeness = ResponseCompletenessEvaluator(model_config=model_config, threshold=3)\n",
    "response_completeness(\n",
    "    response=\"Based on the retrieved documents, the shareholder meeting discussed the operational efficiency of the company and financing options.\",\n",
    "    ground_truth=\"The shareholder meeting discussed the compensation package of the company CEO.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56667c74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Textual Similarity Evaluators\n",
    "\n",
    "\n",
    "These evaluators compare how closely the textual response generated by your AI system matches the response you would expect, typically called the \"ground truth\".\n",
    "- The SimilarityEvaluator uses an \"LLM-as-Judge\" (AI-assisted evaluation) approach to score the metric.\n",
    "- The F1 Score, BLEU, GLEU, ROUGE and METEOR evaluators (NLP-based) use a mathematical approach to score the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1872f9",
   "metadata": {},
   "source": [
    "### 4.1 Similarity Evaluator\n",
    "SimilarityEvaluator measures the degrees of semantic similarity between the generated text and its ground truth with respect to a query. Compared to other text-similarity metrics that require ground truths, this metric focuses on semantics of a response (instead of simple overlap in tokens or n-grams) and also considers the broader context of a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe271d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "similarity = SimilarityEvaluator(model_config=model_config, threshold=3)\n",
    "similarity(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e7260",
   "metadata": {},
   "source": [
    "### 4.2 F1 Score\n",
    "F1ScoreEvaluator measures the similarity by shared tokens between the generated text and the ground truth, focusing on both precision and recall. The F1-score computes the ratio of the number of shared words between the model generation and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator\n",
    "\n",
    "f1_score = F1ScoreEvaluator(threshold=0.5)\n",
    "f1_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16081012",
   "metadata": {},
   "source": [
    "### 4.3 BLEU Score\n",
    "BleuScoreEvaluator computes the BLEU (Bilingual Evaluation Understudy) score commonly used in natural language processing (NLP) and machine translation. It measures how closely the generated text matches the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu_score = BleuScoreEvaluator(threshold=0.3)\n",
    "bleu_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0a01",
   "metadata": {},
   "source": [
    "### 4.4 GLEU Score\n",
    "\n",
    "GleuScoreEvaluator computes the GLEU (Google-BLEU) score. It measures the similarity by shared n-grams between the generated text and ground truth, similar to the BLEU score, focusing on both precision and recall. But it addresses the drawbacks of the BLEU score using a per-sentence reward objective. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "\n",
    "gleu_score = GleuScoreEvaluator(threshold=0.2)\n",
    "gleu_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477569f",
   "metadata": {},
   "source": [
    "### 4.5 ROUGE Score\n",
    "RougeScoreEvaluator computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, a set of metrics used to evaluate automatic summarization and machine translation. It measures the overlap between generated text and reference summaries. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_L, precision_threshold=0.6, recall_threshold=0.5, f1_score_threshold=0.55) \n",
    "rouge(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c3d9d",
   "metadata": {},
   "source": [
    "### 4.6 METEOR Score\n",
    "MeteorScoreEvaluator measures the similarity by shared n-grams between the generated text and the ground truth, similar to the BLEU score, focusing on precision and recall. But it addresses limitations of other metrics like the BLEU score by considering synonyms, stemming, and paraphrasing for content alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor_score = MeteorScoreEvaluator(threshold=0.9)\n",
    "meteor_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d223752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Explore Custom Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9843c29",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.1 Code-Based Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator as a function to calculate response length\n",
    "def response_length(response, **kwargs):\n",
    "    return len(response)\n",
    "\n",
    "# Custom class based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self._blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        contains_block_word = any(word in answer for word in self._blocklist)\n",
    "        return {\"score\": contains_block_word}\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"worst\", \"terrible\"])\n",
    "\n",
    "# Test custom evaluator 1\n",
    "result = response_length(\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 2\n",
    "result = blocklist_evaluator(answer=\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 3\n",
    "result = blocklist_evaluator(answer=\"This is a bad idea.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202d615",
   "metadata": {},
   "source": [
    "### 5.2 Prompt-Based Evaluator\n",
    "To build your own prompt-based large language model evaluator or AI-assisted annotator, you can create a custom evaluator based on a prompt template. [Learn more here](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/custom-evaluators). You can also create a custom grader using the Azure OpenAI grader with custom prompts. [See example here](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/evaluation/azure-ai-evaluation/samples/aoai_score_model_grader_sample.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107224d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Run Multiple Evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70075c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    ")\n",
    "\n",
    "# Create evaluators\n",
    "content_safety_evaluator = ContentSafetyEvaluator( azure_ai_project=azure_ai_project, credential=credential)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"00-data/02-data.jsonl\",\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "    },\n",
    "    evaluation_name=\"02-quality-evaluators\",\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${data.response}\"}},\n",
    "        \"coherence\": {\"column_mapping\": {\"response\": \"${data.response}\", \"query\": \"${data.query}\"}},\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "    },\n",
    "\n",
    "    # Specify the azure_ai_project to push results to portal\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    \n",
    "    # Specify the output path to push results also to local file\n",
    "    output_path=\"./02-quality-evaluators.results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde8ca4",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 3.1 View Results Online\n",
    "\n",
    "Just as before, you can now view the results of the multi-evaluator run using the Evaluation tab in the Azure AI Foundry Studio. Here is what you should see:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ce3da",
   "metadata": {},
   "source": [
    "#### Quality Evaluation\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-02-portal-quality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e146493",
   "metadata": {},
   "source": [
    "### 3.2 View Results Locally\n",
    "\n",
    "Just like before, you can see the results of the multi-evaluator run stored in a local json file at `02-quality-evaluators.results.json`. \n",
    "- Open the file in the VS Code editor\n",
    "- Right click and select \"Format Document\" to read the results better\n",
    "- Observe the metrics collected for each row - these are the multiple evaluators running on the same sample prompt/response pair\n",
    "- Scroll down to the end of the file to see the summary of the evaluation run - get a sense of the overall metrics for the run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686630d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ | Congratulations!\n",
    "\n",
    "You have successfully completed the second lab in this module and got hands-on experience with a core subset of the the built-in quality evaluators. You also got a sense of how to create and run a custom evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dceed0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
